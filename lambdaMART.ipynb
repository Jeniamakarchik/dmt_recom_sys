{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Resources\n",
    "\n",
    "- [A Practical Guide to LambdaMART in LightGbm](https://medium.datadriveninvestor.com/a-practical-guide-to-lambdamart-in-lightgbm-f16a57864f6)\n",
    "- [lightgbm.LGBMRanker Documentation](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRanker.html#)\n",
    "- [LightGBM Parameter Tuning Guide](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)\n",
    "- [How To Use Optuna to Tune LightGBM Hyperparameters](https://forecastegy.com/posts/how-to-use-optuna-to-tune-lightgbm-hyperparameters/)\n",
    "- [Optuna](https://optuna.readthedocs.io/en/stable/tutorial/index.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- Model --------------- #\n",
    "# k for NDCG@k\n",
    "K = 5    \n",
    "# Number of iterations/trees for LightGBM              \n",
    "NUM_ITERATIONS = 1000 # 1000\n",
    "\n",
    "# --------------- Tuning --------------- #\n",
    "# number of trials for Optuna\n",
    "N_TRIALS = 10   # 30   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import pickle\n",
    "\n",
    "import psutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from data import read_processed_train, read_processed_val, read_processed_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = read_processed_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = read_processed_val(num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val2_df = read_processed_val(num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = read_processed_test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Definitions**\n",
    "- Take in train_df or val_df and modify the target column\n",
    "- All columns can be used\n",
    "- Do not change the row order\n",
    "- **target must be an integer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_target(df):\n",
    "    \"\"\"\n",
    "    Applies the weighting given in the assignment (no discounting)\n",
    "    \"\"\"\n",
    "    df['target'] = 0\n",
    "\n",
    "    mask = df['click_bool'] == 1\n",
    "    df.loc[mask, 'target'] = 1\n",
    "\n",
    "    mask = df['booking_bool'] == 1\n",
    "    df.loc[mask, 'target'] = 5\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add others..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Functions**\n",
    "\n",
    "- Apply one of the above functions to train_df and val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choice of target function\n",
    "target_func = base_target\n",
    "\n",
    "# ---------------------------------- #\n",
    "\n",
    "# Apply target function\n",
    "train_df = target_func(train_df)\n",
    "val_df = target_func(val_df)\n",
    "val2_df = target_func(val2_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Defintions**\n",
    "- Take in a dataframe and modify/add columns\n",
    "- Must be applicable to train_df, val_df and test_df\n",
    "- Do not change the row order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hour_day_month_year(df):\n",
    "    \"\"\"\n",
    "    Function to create day, month, year columns from date_time column.\n",
    "    \"\"\"\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "\n",
    "    df['hour'] = df['date_time'].dt.hour\n",
    "    df['day'] = df['date_time'].dt.day\n",
    "    df['month'] = df['date_time'].dt.month\n",
    "    df['year'] = df['date_time'].dt.year\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features that will receive a rank column\n",
    "ranked_cols = ['prop_starrating', 'prop_review_score', 'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price', 'price_usd', 'orig_destination_distance']\n",
    "\n",
    "def add_ranks(df):\n",
    "    \"\"\"\n",
    "    Function to add rank of certain features within each srch_id. Values are\n",
    "    ranked in ascending order, with ties receiving the same rank.\n",
    "    \"\"\"\n",
    "    for ranked_col in ranked_cols:\n",
    "        df[f'{ranked_col}_rank'] = df.groupby('srch_id')[ranked_col].rank(method='dense')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starrating_review_diff(df):\n",
    "    \"\"\"\n",
    "    Function to create absolute difference between 'prop_starrating' and 'prop_review_score'.\n",
    "    \"\"\"\n",
    "    df['starrating_review_diff'] = abs(df['prop_starrating'] - df['prop_review_score'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_adults_children(df):\n",
    "    \"\"\"\n",
    "    Function to create sum of adults and children counts\n",
    "    \"\"\"\n",
    "    df['sum_adults_children'] = df['srch_adults_count'] + df['srch_children_count']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate click and booking probability for each property\n",
    "\n",
    "# Join train and val sets\n",
    "train_val_df = pd.concat([train_df, val_df, val2_df], ignore_index=True)\n",
    "\n",
    "# Probability of click and book for each prop_id\n",
    "train_val_df['click_prob'] = train_val_df.groupby('prop_id')['click_bool'].transform('mean')\n",
    "train_val_df['booking_prob'] = train_val_df.groupby('prop_id')['booking_bool'].transform('mean')\n",
    "\n",
    "# Create lookup tables\n",
    "prob_lookup = train_val_df[['prop_id', 'click_prob', 'booking_prob']].drop_duplicates().set_index('prop_id')\n",
    "\n",
    "def add_probabilities(df):\n",
    "    \"\"\"\n",
    "    Function to add click and booking probabilities to each row. Unseen prop_ids\n",
    "    are given -1.\n",
    "    \"\"\"\n",
    "    df = df.join(prob_lookup, on='prop_id')\n",
    "    df[['click_prob', 'booking_prob']] = df[['click_prob', 'booking_prob']].fillna(-1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jenia's features\n",
    "\n",
    "def log_price(df):\n",
    "    \"\"\"\n",
    "    Function to take log of price_usd column\n",
    "    \"\"\"\n",
    "    df['log_price_usd'] = np.log(df['price_usd'] + 1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add others..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Functions**\n",
    "\n",
    "- Apply some combination of the above functions to train_df, val_df and test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of functions to apply in order\n",
    "# feature_functions = [hour_day_month_year, add_ranks, starrating_review_diff, sum_adults_children, add_probabilities, log_price]\n",
    "feature_functions = [hour_day_month_year, add_ranks, starrating_review_diff, sum_adults_children, add_probabilities, log_price]\n",
    "\n",
    "# ---------------------------------- #\n",
    "\n",
    "# Apply listed functions\n",
    "for func in feature_functions:\n",
    "    train_df = func(train_df)\n",
    "    val_df = func(val_df)\n",
    "    val2_df = func(val2_df)\n",
    "    test_df = func(test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jenia's normalisation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by_cols_norm = [\n",
    "#     'srch_id',\n",
    "#     'srch_destination_id',\n",
    "#     # 'srch_booking_window',\n",
    "#     'prop_id',\n",
    "#     'prop_country_id',\n",
    "#     # 'month',\n",
    "#     'site_id'\n",
    "# ]\n",
    "\n",
    "# cols = [\n",
    "#     # 'price_usd',\n",
    "#     'log_price_usd',\n",
    "#     'prop_review_score',\n",
    "#     'prop_location_score1',\n",
    "#     'prop_location_score2',\n",
    "#     'prop_log_historical_price'\n",
    "# ]\n",
    "\n",
    "# def fit(df, by_col, columns):\n",
    "#     print(f'Fitting means ans stds... - {by_col}')\n",
    "#     all_columns = [by_col] + columns\n",
    "#     groups = train_df[all_columns].groupby(by_col)\n",
    "\n",
    "#     means = groups.mean()\n",
    "#     stds = groups.std()\n",
    "\n",
    "#     return dict(zip(means.index, means.values)), dict(zip(stds.index, stds.values))\n",
    "\n",
    "# def normalise_by_cols(df, means, stds, by_col, columns):\n",
    "#     print('Transformong columns...')\n",
    "#     for idx, col in enumerate(columns):\n",
    "#         print(col)\n",
    "#         upd_colname = f'norm_{col}_{by_col}'\n",
    "\n",
    "#         df[upd_colname] = df[by_col].map(lambda x: -means[x][idx]  if x in means else 0)\n",
    "#         df[upd_colname] = df[upd_colname] + df[col]\n",
    "#         df[upd_colname] = df[upd_colname] / df[by_col].map(lambda x: -stds[x][idx] if x in stds else 1)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# for by_col in by_cols_norm:\n",
    "#     means, stds = fit(train_df, by_col, cols)\n",
    "\n",
    "#     train_df = normalise_by_cols(train_df, means, stds, by_col, cols)\n",
    "#     # val_a_df = normalise_by_cols(val_a_df, means, stds, by_col, cols)\n",
    "#     # val_b_df = normalise_by_cols(val_b_df, means, stds, by_col, cols)\n",
    "#     val_df = normalise_by_cols(val_df, means, stds, by_col, cols)\n",
    "#     test_df = normalise_by_cols(test_df, means, stds, by_col, cols)\n",
    "\n",
    "#     print('---')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cat_cols = ['site_id', 'visitor_location_country_id', 'prop_country_id', 'prop_starrating', 'prop_brand_bool', 'promotion_flag', 'srch_destination_id', 'srch_saturday_night_bool', 'random_bool', 'month', 'year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specify_categorical(df):\n",
    "    \"\"\"\n",
    "    Function to explicitly specify categorical variables.\n",
    "    \"\"\"\n",
    "    df_cols = list(df.columns)\n",
    "\n",
    "    for col in df_cols:\n",
    "        if col in all_cat_cols:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = specify_categorical(train_df)\n",
    "val_df = specify_categorical(val_df)\n",
    "val2_df = specify_categorical(val2_df)\n",
    "test_df = specify_categorical(test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing unwanted columns and preparing the data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify additional columns to remove\n",
    "add_drop_cols = ['srch_id', 'date_time', 'prop_id']\n",
    "\n",
    "# ---------------------------------- #\n",
    "\n",
    "# Remove columns that won't be used as features\n",
    "train_only_cols = ['position', 'click_bool', 'booking_bool', 'target'] + add_drop_cols\n",
    "\n",
    "# Train data\n",
    "group_train = train_df.groupby(\"srch_id\")[\"srch_id\"].count().to_numpy()\n",
    "X_train = train_df.drop(columns=train_only_cols)\n",
    "y_train = train_df['target'].astype(int)\n",
    "\n",
    "# Validation data\n",
    "group_val = val_df.groupby(\"srch_id\")[\"srch_id\"].count().to_numpy()\n",
    "X_val = val_df.drop(columns=train_only_cols)\n",
    "y_val = val_df['target'].astype(int)\n",
    "\n",
    "# Validation 2 data\n",
    "group_val2 = val2_df.groupby(\"srch_id\")[\"srch_id\"].count().to_numpy()\n",
    "X_val2 = val2_df.drop(columns=train_only_cols)\n",
    "y_val2 = val2_df['target'].astype(int)\n",
    "\n",
    "# Test data\n",
    "X_test = test_df.drop(columns=add_drop_cols).to_numpy(copy=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding custom evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dcg_at_k(r, k):\n",
    "#     r = np.asfarray(r)[:k]\n",
    "#     if r.size:\n",
    "#         return np.sum(np.subtract(np.power(2, r), 1) / np.log2(np.arange(2, r.size + 2)))\n",
    "    \n",
    "#     return 0.\n",
    "\n",
    "# def ndcg_at_k(y_true, y_pred, k=5):\n",
    "#     # Calculate the ideal NDCG\n",
    "#     rank_true = sorted(y_true, reverse=True)\n",
    "#     dcg_max = dcg_at_k(rank_true, k)\n",
    "\n",
    "#     # Calculate the NDCG for the predictions\n",
    "#     rank_pred = [x for _, x in sorted(zip(y_pred, y_true), reverse=True)]\n",
    "#     dcg_pred = dcg_at_k(rank_pred, k)\n",
    "\n",
    "#     # Return the NDCG\n",
    "#     return dcg_pred / dcg_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_eval_metric(y_true, y_pred, weight, group):\n",
    "#     # Calculate the NDCG for each group\n",
    "#     ndcg = []\n",
    "#     start = 0\n",
    "#     for grp in group:\n",
    "#         end = start + grp\n",
    "#         y_true_group = y_true[start:end]\n",
    "#         y_pred_group = y_pred[start:end]\n",
    "\n",
    "#         ndcg.append(ndcg_at_k(y_true_group, y_pred_group))\n",
    "\n",
    "#         start = end\n",
    "\n",
    "#     # Calculate the overall NDCG\n",
    "#     ndcg_avg = np.average(ndcg, weights=weight)\n",
    "#     return 'cust_ndcg', ndcg_avg, True # (eval_name, eval_result, is_higher_better)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder to store the results of the tuning\n",
    "tune_folder_path = Path(\"./tuned_models/\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "tune_folder_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constant hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_params = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"boosting_type\": \"dart\",\n",
    "    \"metric\": \"ndcg\",\n",
    "    \"n_estimators\": NUM_ITERATIONS,\n",
    "    'max_depth': -1,\n",
    "    \"importance_type\": \"gain\",\n",
    "    \"label_gain\": [i for i in range(max(y_train.max(), y_val.max()) + 1)],\n",
    "    \"bagging_freq\": 1,\n",
    "    \"n_jobs\": psutil.cpu_count(logical=False) - 1,\n",
    "    \"verbosity\": -1,\n",
    "    # new params\n",
    "    # \"early_stopping_round\": int(NUM_ITERATIONS/10),  # stops if 10% of iterations val score doesn't improve\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the objective"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes:\n",
    "- num_leaves [10, 35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    tuned_params = {\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 35),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.01, log=True),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 100, 5000),\n",
    "        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0.01, 1, log=True),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.5, 0.9),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.01, 0.5),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 5),\n",
    "    }\n",
    "\n",
    "    params = const_params | tuned_params\n",
    "\n",
    "    model = lgb.LGBMRanker(**params)\n",
    "    model.fit(\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        group=group_train,\n",
    "        eval_set=[(X_train, y_train),(X_val, y_val)],\n",
    "        eval_group=[group_train, group_val],\n",
    "        eval_at=[K], # k for NDCG@k\n",
    "        verbose=False,\n",
    "      )\n",
    "    \n",
    "    # Save the trained model\n",
    "    with open(tune_folder_path / f\"trial_{trial.number}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    best_val_score = model.best_score_['valid_1'][f'ndcg@{K}']\n",
    "    return best_val_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimise the objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=N_TRIALS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the best model (and rename it)\n",
    "for trial in study.trials:\n",
    "    if trial.number != study.best_trial.number:\n",
    "        os.remove(tune_folder_path / f\"trial_{trial.number}.pkl\")\n",
    "    else:\n",
    "        os.rename(tune_folder_path / f\"trial_{trial.number}.pkl\", tune_folder_path / \"best_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best hyperparameters\n",
    "with open(tune_folder_path / \"best_params.json\", \"w\") as f:\n",
    "    best_params = const_params | study.best_params\n",
    "    json.dump(best_params, f)\n",
    "\n",
    "# Save some other relevant information\n",
    "with open(tune_folder_path / \"info.txt\", \"w\") as f:\n",
    "    f.write(f\"{10 * '-'} Global constants {10 * '-'}\\n\\n\")\n",
    "    f.write(f\"NUM_ITERATIONS = {NUM_ITERATIONS}\\n\")\n",
    "    f.write(f\"K = {K}\\n\")\n",
    "    f.write(f\"N_TRIALS = {N_TRIALS}\\n\\n\")\n",
    "    f.write(f\"{10 * '-'} Features {10 * '-'}\\n\\n\")\n",
    "    f.write(f\"{list(X_train.columns)}\\n\\n\")\n",
    "    f.write(f\"{10 * '-'} Study Info {10 * '-'}\\n\\n\")\n",
    "    f.write(f\"Best value: {study.best_value}\\n\")\n",
    "    f.write(f\"Best trial number: {study.best_trial.number}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the model (choose one)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a new model with hand-picked hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#         # Constant parameters\n",
    "#         \"objective\": \"lambdarank\",\n",
    "#         \"boosting_type\": \"dart\",\n",
    "#         \"metric\": \"ndcg\",\n",
    "#         \"n_estimators\": NUM_ITERATIONS, \n",
    "#         \"importance_type\": \"gain\",\n",
    "#         \"label_gain\": [i for i in range(max(y_train.max(), y_val.max()) + 1)],\n",
    "#         \"bagging_freq\": 1,\n",
    "#         \"n_jobs\": psutil.cpu_count(logical=False),\n",
    "#         \"verbosity\": -1,\n",
    "#         # Tuning parameters\n",
    "#         \"num_leaves\": 32,\n",
    "#         'max_depth': -1,\n",
    "#         \"learning_rate\": 0.01,\n",
    "#         \"subsample\": 0.8,\n",
    "#         \"colsample_bytree\": 0.8,\n",
    "#         \"min_data_in_leaf\": 50,\n",
    "#     }\n",
    "\n",
    "model = lgb.LGBMRanker(\n",
    "        objective=\"lambdarank\",\n",
    "        metric=\"ndcg\",\n",
    "        n_estimators=NUM_ITERATIONS,\n",
    "        learning_rate=0.12,\n",
    "        max_position=5,\n",
    "        label_gain=[0, 1, 5],\n",
    "        boosting='dart',\n",
    "    )\n",
    "# model = lgb.LGBMRanker(**params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a new model with the most recently tuned hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"./tuned_models\")\n",
    "most_recent_dir = [str(d)[-15:] for d in sorted(root.iterdir()) if d.is_dir()][-1]\n",
    "\n",
    "with open(root / most_recent_dir / \"best_params.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "model = lgb.LGBMRanker(**best_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the best model from the most recent tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"./tuned_models\")\n",
    "most_recent_dir = [str(d)[-15:] for d in sorted(root.iterdir()) if d.is_dir()][-1]\n",
    "model_path = root / most_recent_dir / \"best_model.pkl\"\n",
    "\n",
    "model = pickle.load(open(model_path, \"rb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lgb.LGBMRanker model however you like\n",
    "root = Path(\"./tuned_models\")\n",
    "most_recent_dir = [str(d)[-15:] for d in sorted(root.iterdir()) if d.is_dir()][-1]\n",
    "\n",
    "with open(root / most_recent_dir / \"best_params.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "\n",
    "best_params['n_estimators'] = 500\n",
    "\n",
    "model = lgb.LGBMRanker(**best_params)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"/Users/nathanjones/Development/VU/DMT/dmt_recom_sys/trained_models/20230527_230454/model.pkl\"\n",
    "# model = pickle.load(open(model_path, \"rb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting the model** (skip if using a trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "      X=X_train,\n",
    "      y=y_train,\n",
    "      group=group_train,\n",
    "      eval_set=[(X_train, y_train),(X_val, y_val)],\n",
    "      eval_group=[group_train, group_val],\n",
    "      eval_at=K, # k for NDCG@k\n",
    "      early_stopping_rounds=200,\n",
    "      # eval_metric=custom_eval_metric,\n",
    "      )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_metric = lgb.plot_metric(model, figsize = (12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_importance = lgb.plot_importance(model, xlabel='Importance (Gain)', figsize = (12,8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating\n",
    "\n",
    "Use the second (unseen) validation set to estimate the final NDCG@5 of the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val2_copy = X_val2.to_numpy(copy=True)\n",
    "\n",
    "prediction = model.predict(X_val2_copy)\n",
    "val2_df['prediction'] = prediction\n",
    "pred_rank = val2_df.sort_values(by=['srch_id', 'prediction'], ascending=[True, False])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate NDCG@5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(df, k):\n",
    "    # Add discounted gain column\n",
    "    df['i'] = df.groupby('srch_id').cumcount() + 1\n",
    "    df['dg'] = ((2 ** df['target']) - 1) / np.log2(df['i'] + 1)\n",
    "\n",
    "    # calculate dcg scores for each srch_id\n",
    "    mask = df['i'] <= k\n",
    "    return df[mask].groupby('srch_id')['dg'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the DCG scores for the predicted ranking\n",
    "pred_dcg = dcg(pred_rank, K)\n",
    "\n",
    "# Finds the DCG scores for the true ranking\n",
    "true_rank = val2_df.sort_values(by=['srch_id', 'target'], ascending=[True, False]).reset_index()\n",
    "true_dcg = dcg(true_rank, K)\n",
    "\n",
    "# Calculate avergae NDCG\n",
    "ndcg_val2 = (pred_dcg['dg'] / true_dcg['dg']).mean()\n",
    "print(f\"Val 2 NDCG@{K} = {ndcg_val2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)\n",
    "test_df['prediction'] = prediction\n",
    "test_df = test_df.sort_values(by=['srch_id', 'prediction'], ascending=[True, False])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder to store the results of the training\n",
    "train_folder_path = Path(\"./trained_models/\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "train_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open(train_folder_path / \"model.pkl\", \"wb\"))\n",
    "\n",
    "# Save the parameters\n",
    "with open(train_folder_path / \"params.txt\", \"w\") as f:\n",
    "    f.write(json.dumps(model.get_params()))\n",
    "\n",
    "# Save the feature names\n",
    "with open(train_folder_path / \"feature_names.txt\", \"w\") as f:\n",
    "    f.write(str(list(X_train.columns)))\n",
    "\n",
    "# Save the figures\n",
    "ax_metric.figure.savefig(train_folder_path / \"learning_curve.pdf\", bbox_inches=\"tight\")\n",
    "ax_importance.figure.savefig(train_folder_path / \"feature_importance.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "# Save best evaluation scores\n",
    "with open(train_folder_path / \"best_scores.txt\", \"w\") as f:\n",
    "    f.write(\"Best training score: \" + str(model.best_score_['training'][f'ndcg@{K}']) + \"\\n\")\n",
    "    f.write(\"Best validation 1 score: \" + str(model.best_score_['valid_1'][f'ndcg@{K}']) + \"\\n\")\n",
    "    f.write(\"Best validation 2 score: \" + str(ndcg_val2) + \"\\n\")\n",
    "\n",
    "# Save the submission file\n",
    "submission_file = train_folder_path / \"submission.csv\"\n",
    "test_df[['srch_id', 'prop_id']].to_csv(submission_file, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
